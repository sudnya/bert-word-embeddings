

Goal 1: Implement a LM (embeddings) training pipeline

1. Create a vocab that can handle every word (e.g. using a tokenizer with fallback)
2. Build a simple model for predicting the word using context (e.g. forward/backward using BERT)
3. Add a class based implementation of the vocab
4. Train on a basic dataset, report perplexity

Goal 2: Use inverse reinforcement learning to discover rewards from different datasets

    Why: Answer: what do (lots of) users care about?
        Hypothesis: Quite a few different things, perhaps including some things that we don't already know about.





